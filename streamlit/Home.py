import streamlit as st
from pathlib import Path
from ui.sidebar import render_sidebar
from ui.utils import detect_group
import json
from datetime import datetime
import pandas as pd
import subprocess # <--- CORRECCIÃ“N
import os # <--- CORRECCIÃ“N

# Welcome page content based on project README
st.set_page_config(page_title="Astrocitos 3D - AnÃ¡lisis", page_icon="ðŸ§ ", layout="wide")

st.title("AnÃ¡lisis MorfolÃ³gico 3D de Astrocitos")
render_sidebar(show_calibration=True)

st.markdown(
    """
    **Proyecto:** ReconstrucciÃ³n, segmentaciÃ³n y anÃ¡lisis morfolÃ³gico 3D de astrocitos (GFAP) y nÃºcleos (DAPI) a partir de imÃ¡genes `.lif`/`.tif`.

    **TecnologÃ­as:** Python, Streamlit, Napari, Cellpose, scikit-image, SciPy, Skan

    ---

    ### Flujo del pipeline
    El flujo de procesamiento unificado (4 pasos):
    1. **CalibraciÃ³n (01):** Lectura de `calibration.json` para Âµm/px â†’ MÃ¡scara Otsu de fondo DAPI
    2. **SegmentaciÃ³n (02):** Cellpose 3D â†’ `02_cellpose_mask.tif`
    3. **Filtrado (03):** 
       - Filtrado GFAP relativo (StdDev sobre fondo) â†’ `03_gfap_filtered_mask.tif`
       - Filtrado por tamaÃ±o fÃ­sico â†’ `04_final_astrocytes_mask.tif`
    4. **EsqueletizaciÃ³n + Sholl (04):** **Pipeline 2D unificado**
       - ProyecciÃ³n 3Dâ†’2D (max projection)
       - Territorios Voronoi con zona de exclusiÃ³n
       - EsqueletizaciÃ³n 2D por territorio con conexiÃ³n de fragmentos
       - AnÃ¡lisis de Sholl 2D nativo integrado (SKAN)
       - Genera `05_skeleton_labels_2d.tif`, `sholl_2d_native.csv`, `sholl_summary.csv`, `sholl_rings_2d_native.json`
    
    **Ventajas del flujo 2D nativo:**
    - âœ… ResoluciÃ³n XY completa (0.38 Âµm) sin degradaciÃ³n
    - âœ… Sholl 2D mÃ¡s preciso y eficiente
    - âœ… Territorios astrocitarios bien definidos para preparados planos
    - âœ… MÃ¡s rÃ¡pido (~10x) que esqueletizaciÃ³n 3D con remuestreo isotrÃ³pico
    """
)

st.info("UsÃ¡ el menÃº lateral para navegar el pipeline. La calibraciÃ³n y parÃ¡metros globales viven en streamlit/calibration.json; los resultados por preparado quedan en data/processed/<preparado>/.")

# Inspector/Editor de configuraciÃ³n unificada
root = Path(__file__).resolve().parents[1]
calib_path = root / "streamlit" / "calibration.json"

st.markdown("---")
st.subheader("Inspector de configuraciÃ³n global (calibration.json)")

def _load_calib() -> dict:
    if calib_path.exists():
        try:
            return json.loads(calib_path.read_text())
        except Exception:
            return {}
    return {}

def _save_calib(data: dict):
    calib_path.parent.mkdir(parents=True, exist_ok=True)
    calib_path.write_text(json.dumps(data, indent=2))

cfg = _load_calib()

colv1, colv2 = st.columns(2)
with colv1:
    st.caption("Vista actual (solo lectura)")
    st.json(cfg or {})
with colv2:
    st.caption("Editar como JSON (avanzado)")
    raw = st.text_area("Contenido de calibration.json", value=json.dumps(cfg, indent=2), height=260)
    bcol1, bcol2 = st.columns(2)
    with bcol1:
        if st.button("ðŸ’¾ Guardar cambios"):
            try:
                new_cfg = json.loads(raw) if raw.strip() else {}
            except Exception as e:
                st.error(f"JSON invÃ¡lido: {e}")
            else:
                _save_calib(new_cfg)
                st.success("Cambios guardados. RecargÃ¡ la pÃ¡gina (F5) para aplicar.")
                st.rerun()
    with bcol2:
        if st.button("ðŸ§° Exportar backup con timestamp"):
            ts = datetime.now().strftime("%Y%m%d-%H%M%S")
            backup_path = calib_path.with_name(f"calibration.backup-{ts}.json")
            backup_path.write_text(json.dumps(cfg, indent=2))
            st.info(f"Backup exportado: {backup_path.relative_to(root)}")

# --- Dashboard del pipeline por preparado (ACTUALIZADO) ---
st.markdown("---")
st.subheader("Estado del pipeline por preparado")

raw_dir = root / "data" / "raw"
files = sorted([p for p in raw_dir.rglob("*.tif")] + [p for p in raw_dir.rglob("*.tiff")])

if files:
    stem_to_group_all = {p.stem: detect_group(p, root) for p in files}

    group_filter = st.session_state.get("group_filter", "Todos")
    files_filtered = files if group_filter == "Todos" else [p for p in files if stem_to_group_all.get(p.stem, "CTL") == group_filter]

    rows = []
    for p in files_filtered:
        od = root / "data" / "processed" / p.stem
        rows.append({
            "prepared": p.stem,
            "group": stem_to_group_all.get(p.stem, "CTL"),
            "02_Nucleos": (od/"02_cellpose_mask.tif").exists(),
            "03_Metricas_Nucleo": (od/"03_nucleus_metrics.csv").exists(),
            "04_Astrocitos": (od/"04_final_astrocytes_mask.tif").exists(),
            "04_Skeleton_2D": (od/"05_skeleton_labels_2d.tif").exists(),
            "04_Sholl_2D": (od/"sholl_2d_native.csv").exists(),
        })
    
    df_state = pd.DataFrame(rows)
    
    if not df_state.empty:
        def mark(col):
            return df_state[col].map(lambda v: "âœ…" if bool(v) else "â€”")
        
        view = df_state.copy()
        # Actualizar los nombres de las columnas para el dashboard
        cols_to_check = [
            "02_Nucleos", "03_Metricas_Nucleo", "04_Astrocitos", 
            "04_Skeleton_2D", "04_Sholl_2D"
        ]
        for c in cols_to_check:
            if c in view.columns:
                view[c] = mark(c)
                
        st.dataframe(view.set_index(["prepared","group"]), use_container_width=True)
    else:
        st.info("No hay preparados para mostrar con el filtro actual.")
else:
    st.info("No se encontraron archivos en data/raw. CargÃ¡ tu dataset para ver el dashboard.")